<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Working with PRISM data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>


<meta name="citation_title" content="Working with PRISM data">
<meta name="citation_author" content="Travis Zalesky">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Prism: Download data from the oregon prism project;,citation_author=Edmund M. Hart;,citation_author=Kendon Bell;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_fulltext_html_url=https://github.com/ropensci/prism;,citation_doi=10.5281/zenodo.33663;">
<meta name="citation_reference" content="citation_title=Terra: Spatial data analysis;,citation_author=Robert J. Hijmans;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=terra;">
<meta name="citation_reference" content="citation_title=R: A language and environment for statistical computing;,citation_author=R Core Team;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://www.R-project.org/;">
<meta name="citation_reference" content="citation_title=Dates and times made easy with lubridate;,citation_author=Garrett Grolemund;,citation_author=Hadley Wickham;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_fulltext_html_url=https://www.jstatsoft.org/v40/i03/;,citation_issue=3;,citation_volume=40;,citation_journal_title=Journal of Statistical Software;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Working with PRISM data</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Author</div>
          <div class="quarto-title-meta-heading">Affiliation</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Travis Zalesky </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        The University of Arizona
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      <div class="quarto-alternate-formats"><div class="quarto-title-meta-heading">Other Formats</div><div class="quarto-title-meta-contents"><p><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></p></div><div class="quarto-title-meta-contents"><p><a href="index-meca.zip" data-meca-link="true"><i class="bi bi-archive"></i>MECA Bundle</a></p></div></div></div>
    </div>



    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract"><span class="header-section-number">1</span> Abstract</a>
  <ul class="collapse">
  <li><a href="#objectives" id="toc-objectives" class="nav-link" data-scroll-target="#objectives"><span class="header-section-number">1.1</span> Objectives</a></li>
  </ul></li>
  <li><a href="#sec-prism" id="toc-sec-prism" class="nav-link" data-scroll-target="#sec-prism"><span class="header-section-number">2</span> Downloading PRISM data</a>
  <ul class="collapse">
  <li><a href="#warning" id="toc-warning" class="nav-link" data-scroll-target="#warning"><span class="header-section-number">2.1</span> WARNING!</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup"><span class="header-section-number">2.2</span> Setup</a></li>
  <li><a href="#defining-variables-of-interest" id="toc-defining-variables-of-interest" class="nav-link" data-scroll-target="#defining-variables-of-interest"><span class="header-section-number">2.3</span> Defining variables of interest</a></li>
  <li><a href="#downloading-the-data" id="toc-downloading-the-data" class="nav-link" data-scroll-target="#downloading-the-data"><span class="header-section-number">2.4</span> Downloading the data</a></li>
  <li><a href="#sec-rawData" id="toc-sec-rawData" class="nav-link" data-scroll-target="#sec-rawData"><span class="header-section-number">2.5</span> Checking the raw data</a></li>
  </ul></li>
  <li><a href="#sec-terra" id="toc-sec-terra" class="nav-link" data-scroll-target="#sec-terra"><span class="header-section-number">3</span> Cropping PRISM data to study area</a>
  <ul class="collapse">
  <li><a href="#setup-1" id="toc-setup-1" class="nav-link" data-scroll-target="#setup-1"><span class="header-section-number">3.1</span> Setup</a></li>
  <li><a href="#exploring-the-data" id="toc-exploring-the-data" class="nav-link" data-scroll-target="#exploring-the-data"><span class="header-section-number">3.2</span> Exploring the data</a></li>
  <li><a href="#automated-data-processing" id="toc-automated-data-processing" class="nav-link" data-scroll-target="#automated-data-processing"><span class="header-section-number">3.3</span> Automated data processing</a></li>
  <li><a href="#sec-cropData" id="toc-sec-cropData" class="nav-link" data-scroll-target="#sec-cropData"><span class="header-section-number">3.4</span> Checking the cropped data</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">4</span> Conclusions</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="abstract" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="abstract"><span class="header-section-number">1</span> Abstract</h2>
<p>Parameter-elevation Regressions on Independent Slopes Model (PRISM) data is available from Oregon State University for a variety of common climate variables such as min/max temperature, precipitation, dew point etc. These datasets are extremely useful in a variety of research applications, and they can be conveniently accessed through the <a href="https://prism.oregonstate.edu/">PRISM website</a>, or they can be accessed programmatically through the data api using the R package <a href="https://cran.r-project.org/web/packages/prism/index.html">prism</a>.</p>
<p>While the prism R package is great, the goal of this tutorial, and associated script, is to extend the prism package in an attempt to solve a common problem. PRISM data is delivered as a raster for the contiguous US (at 4Km or 800m resolution), when often only a much smaller extent is needed to answer the research question at hand. Depending on the project, storing a large number of rasters for the whole US may be unnecessary and could massively increase data storage costs. Originally developed for a research project covering the Phoenix Metro. area this script is designed to work with a shapefile defining the area of interest, and includes a variety of functions which will iteratively (1) download the requested data file from PRISM, (2) clip the data to the study area, and (3) save the output clipped raster, preserving all relevant metadata and file structure. The original (US extent) data files are then, optionally, deleted to save storage space.</p>
<section id="objectives" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="objectives"><span class="header-section-number">1.1</span> Objectives</h3>
<ol type="1">
<li>Bulk download data from prism.oregonstate.edu.</li>
<li>Crop data to a given study area using a shapefile.</li>
<li>Save cropped data sets, preserve file metadata and .bil format.</li>
<li>Delete raw data (optional).</li>
</ol>
</section>
</section>
<section id="sec-prism" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="sec-prism"><span class="header-section-number">2</span> Downloading PRISM data</h2>
<section id="warning" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="warning"><span class="header-section-number">2.1</span> WARNING!</h3>
<p><strong>Repeated downloading of files from the PRISM api may result in sanctions, including blocking your IP address. Please be respectfull of these resources.</strong></p>
</section>
<section id="setup" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="setup"><span class="header-section-number">2.2</span> Setup</h3>
<p>As with any R <span class="citation" data-cites="base_R">[<a href="#ref-base_R" role="doc-biblioref">1</a>]</span> script, we must first load the required packages. Initially we will utilize 2 packages, prism and lubridate. The prism <span class="citation" data-cites="prism">[<a href="#ref-prism" role="doc-biblioref">2</a>]</span> package is the primary R package for programmatically interacting with the PRISM api. Lubridate <span class="citation" data-cites="lubridate">[<a href="#ref-lubridate" role="doc-biblioref">3</a>]</span> is a package for working with dates, and will be used for its <code>years()</code> function.</p>
<pre><code> Setup ----
# Packages
# Check if packages are installed, if not downlowad
# PRISM api package 'prism'
if(!require('prism')) {  
  # if package is not available, require() returns FALSE
  install.packages('prism')  # install the package
  library(prism)  # Attach the package
}
# Working with dates 'lubridate', required for year() function
if(!require('lubridate')) {  
  # if package is not available, require() returns FALSE
  install.packages('lubridate')  # install the package
  library(lubridate)  # Attach the package
}</code></pre>
<p>Next we set the working directory (wd). This is the absolute filepath in your machine in which all the files exist, and where data will be downloaded. Relative file paths will begin at the working directory.</p>
<p>Here I have used the R function <code>getwd()</code> to obtain the default working directory from your R session. This is primaraly used to make sharing this script easier and more convenient, but it is highly recommended that you change the var <code>dir.string</code> to an absolute file path representing your preferred file location, as this will be used extensively while downloading and managing your PRISM data.</p>
<pre><code># Set Working Directory (wd)
dir.string &lt;- getwd()  # Update with directory string as needed!
setwd(dir.string)
getwd()  # Check wd</code></pre>
<p>Next we will create a new folder in our wd called “Data”, and set it as the default data download directory for the PRISM api using the <code>prism_set_dl_dir()</code> function.</p>
<pre><code># Create Data folder in wd
dir.create("Data")  # Ignore warning if dir already exists

# Assign prism download directory
# Use the paste() function to concatenate the full file path
# Use full directory path for more rhobust code (less error prone)
prism_set_dl_dir(paste(dir.string, "Data", sep = "/"))</code></pre>
</section>
<section id="defining-variables-of-interest" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="defining-variables-of-interest"><span class="header-section-number">2.3</span> Defining variables of interest</h3>
<p>Now that we have set up our directory, we next have to define the date range and variables of interest. Here I have defined a date range and set of variables for this tutorial, but in the future, this will be the section of code which you will primaraly be responsible for updating to suit the needs of your project.</p>
<section id="setting-a-date-range" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="setting-a-date-range"><span class="header-section-number">2.3.1</span> Setting a date range</h4>
<p>At minimum a date range is required for data to download. Depending on the data set that you are interested in, some PRISM records extend back as far as 1895, and new data is typically made available in near real time. Here we will look for data from the beginning of 2020 through 2022.</p>
<pre><code># Define date range
min.Date &lt;- "2020-01-01"
max.Date &lt;- "2022-12-31"
# For most up to date data available, use max.Date &lt;- lubridate::today()</code></pre>
<p>Additionally, in some instances you may only be interested in a certain months of the year. You may optionally set a range of months here. If no months are defined, the default behavior will be to download all months of the year (if applicable). In this example we will only look at data from April to July (4 to 7).</p>
<pre><code># Optional: list of months to download for the get_prism_monthlies() function
months &lt;- c(4:7)  # Numeric list, not strings!</code></pre>
</section>
<section id="setting-temporal-resolution" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="setting-temporal-resolution"><span class="header-section-number">2.3.2</span> Setting temporal resolution</h4>
<p>PRISM data is available at a four temporal scales, daily, monthly, annual, and 30-year normals. While you are free to experiment with the prism functions <code>get_prism_dailys()</code>, <code>get_prism_monthlys()</code>, <code>get_prism_annual()</code>, and <code>get_prism_normals()</code> the scripts included with this tutorial have already set up the functions so that you can quickly and eficiently get the data you need with a minimum number of changes.</p>
<p>Some things to consider at this point, firstly, in some instances you may want a dis-continuous data set, such as dialy observations for the month of July from 1999 to 2020. Unfortunately none of these scripts are currently able to accept a dis-continusous date range. Your choices are to either download the full continuous dataset and subset the data after it has been downloaded, break the dataset into smaller blocks of continuous dates and run multiple times, or write your own custom script for handling date breaks. Also, the way that I have written the <code>get_prism_monthlys()</code> and <code>get_prism_normals()</code> functions may sometimes result in unexpected behavior, depending on the <code>min.date</code> and <code>max.date</code> provided. In short, the function extracts the year from the date variables, and the rest of the date bounds are disregarded. Therefore you may get more data than you expect, but the error will always be inclusive (as opposed to not downloading requested data).</p>
<pre><code># Define temporal resolution
# tRes must be one of "dailys", "monthlys", "annual", "normals"
tRes &lt;- "monthlys"</code></pre>
</section>
<section id="setting-the-climtic-variables" class="level4" data-number="2.3.3">
<h4 data-number="2.3.3" class="anchored" data-anchor-id="setting-the-climtic-variables"><span class="header-section-number">2.3.3</span> Setting the climtic variables</h4>
<p>There are 11 climactic variables available through PRISM, of which seven are available through the api. These are precipitation (ppt), mean temperature (tmean), minimum temperature (tmin), maximum temperature (tmax), mean dew point temperature (tdmean), minimum vapor pressure deficit (vpdmin), and maximum vapor pressure deficit (vpdmax). Any combination of these variables may be assigned to the <code>prismVars</code> variable, and all of the requested climactic data will be downloaded.</p>
<pre><code># Define data variables
# Must be one (or several) of "ppt", "tmean", "tmin", "tmax", "tdmean", "vpdmin", "vpdmax"
# Use c("var1", "var2", etc...) for multiple variables
prismVars &lt;- c("ppt", "tdmean")
# Missing variables not available through the PRISM api = 
#   "Soltotal", "Solslope", "Solclear", "Soltrans"</code></pre>
</section>
</section>
<section id="downloading-the-data" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="downloading-the-data"><span class="header-section-number">2.4</span> Downloading the data</h3>
<p>Now that we have defined the data to download, we can execute the prism functions to connect to the api and commence data downloading. But first we will run a couple of simple checks.</p>
<p>To make sure that the <code>tRes</code> variable is set correctly we will check it against a list. If an issue is detected it will return a custom error message which will warn the user to double check their work.</p>
<pre><code># Check if temporal resolution (tRes) is set correctly.
# If tRes not in list
if (tRes != "dailys" &amp; tRes != "monthlys" &amp; tRes != "annual" &amp; tRes != "normals") {
  # Print a custom error message
  stop('tRes must be one of "dailys", "monthlys", "annual", or "normals". Please ensure your variable is set correctly.')
}</code></pre>
<p>Next, to avoid potentially costly user errors an if statement is called so that only the data of interest is downloaded. This way all of the prism <code>get_prism_...()</code> functions can be prebuilt and live inside our script, enabling us to work quickly, but (hopefully) preventing us from running the wrong function and ending up with way too much data. Here I am only showing the <code>get_prism_monthlies()</code> function which we are interested in. See the full prism.R script accompanying this tutorial to see how all the functions are defined.</p>
<p>Depending on the amount of data requested this function may take from seconds to hours. It includes several convienint progress statements, and is nested inside a timing function. This small example should take about 35 seconds to complete.</p>
<pre><code># If tRes is "monthlys"
# Monthlies ----
if (tRes == "monthlys") {
  system.time({ #Timing function.
    # Check if var "months" is defined
    if (exists("months")) {
      # If exists() = TRUE
      months &lt;- months  # Use months as defined
    } else {
      months &lt;- c(1:12) # else, default to all months of the year.
    }
    # For each prism climactic variable in list of prismVars
    for (prismVar in prismVars) {
      print(prismVar)  # Print the current variable
      # Use the prism::get_prism_monthlys() function to connect to the PRISM api
      get_prism_monthlys(type = prismVar, # dataset to be downloaded
                         # Date range
                         # Use lubridate::years() function to extract the year from the min/max date vars
                         # Generate a regular sequence from min year to max year
                         years = year(seq(as.Date(min.Date), as.Date(max.Date), by="years")),
                         mon = months,  # Months to download
                         keepZip = F) #delete zip folders
      
    } # end for prismVar in prismVars
  }) # end system.time
}</code></pre>
</section>
<section id="sec-rawData" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="sec-rawData"><span class="header-section-number">2.5</span> Checking the raw data</h3>
<p>At this point all of your data should have downloaded into the “Data” folder. You can check on your files using the file explorer (or finder on Mac). We will use a custom function to check our files programmatically and return the total size of files in our directory, this will also allow us to quantify the storage savings after we have cropped our files in <a href="#sec-cropData" class="quarto-xref">Section&nbsp;3.4</a>.</p>
<pre><code>#| label: dataSize-fun
# Optional: Check size of downloaded data
dir_size &lt;- function(path, recursive = TRUE) {
  stopifnot(is.character(path))
  files &lt;- list.files(path, full.names = T, recursive = recursive)
  vect_size &lt;- sapply(files, function(x) file.size(x))
  size_files &lt;- sum(vect_size)
  size_files  # return file size in bytes
}
rawDataSize &lt;- dir_size("Data")/10**6  # bytes to MB
# Print statement
cat(paste("Data folder size on disk =", rawDataSize, "MB", sep = " "))</code></pre>
<p>Approximate size of files on disk = 105 MB.</p>
</section>
</section>
<section id="sec-terra" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-terra"><span class="header-section-number">3</span> Cropping PRISM data to study area</h2>
<p>So far we have been working primaraly with the prism package, and have succeeded in connecting to the PRISM api and eficiently downloading data in bulk. However, this could arguably have been achieved just as efficiently by simply calling the required function from the prism package, without all the extranious setup. Here we will extend the useful application of the prism package by incorporating functions from the terra package <span class="citation" data-cites="terra">[<a href="#ref-terra" role="doc-biblioref">4</a>]</span> to crop the data to a study area, thereby saving ourselves from the enormous data storage costs of maintaining nation wide datasets, when we are only interested in a small region.</p>
<p>In our example we will look at Maricopa county, AZ.</p>
<section id="setup-1" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="setup-1"><span class="header-section-number">3.1</span> Setup</h3>
<p>While contained within the same script for convenience, the code presented in this section is independent from the code presented in <a href="#sec-prism" class="quarto-xref">Section&nbsp;2</a>. You may continue in your R session, or may restart your R session as desired.</p>
<p>Lets begin by loading the terra package.</p>
<pre><code># Packages
# "terra" package for working with geospatial datasets
if(!require('terra')) {  # if package is not available, require() returns FALSE
  install.packages('terra')  # install the package
  library(terra)  # Attach the package
}</code></pre>
</section>
<section id="exploring-the-data" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="exploring-the-data"><span class="header-section-number">3.2</span> Exploring the data</h3>
<p>While we could jump straight into the automated data processing, it is generally a good idea to take a moment to explore your data prior to bulk processing. This will also afford us the opportunity to briefly explore some of the functions of terra, a massively useful package for geospatial data analysis in R!</p>
<p>First, lets just load in a representative data folder, and a raster file. Here I will be using a regular expression (regex) for pattern matching. Regex is largely beyond the scope of this tutorial, but it a wildly useful programmatic tool for advanced users. I will do my best to include plain language explanations for all my regex patterns, but for those who are interested to learn more I highly recommend reading this <a href="https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html">vignette</a> on cran.</p>
<pre><code># Locate Raster files, "bil" extension.
folder1 &lt;- list.files("Data")[1] # Select first data folder in download directory
file1 &lt;- list.files(paste("Data", folder1, sep = "/"), 
                    # pattern = regex, literal "bil.bil" at end of string.
                    pattern = "bil.bil$",
                    # Full file path 
                    full.names = T)
# There should only be one .bil file per data folder
# Get file name from file path
# Use length() function to return the last item in the list (i.e. index = length(list))
name &lt;- strsplit(file1, split = "/")[[1]][length(strsplit(file1, split = "/")[[1]])]</code></pre>
<section id="loading-raster-data" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="loading-raster-data"><span class="header-section-number">3.2.1</span> Loading raster data</h4>
<p>Loading raster data in R is as simple as calling the terra <code>rast()</code> function, with the file path to the data. After loading the raster (i.e.&nbsp;terra “spatRast” object) we can map it using <code>terra::plot()</code>. Note that when terra was loaded the terra <code>plot()</code> function automatically masked the base R <code>plot()</code> function and can be called simply <code>plot()</code>.</p>
<pre><code># Load raster data, plot
rast1 &lt;- rast(file1)  # terra spatRaster object
plot(rast1, main = name)  # Check if data loaded correctly</code></pre>
</section>
<section id="loading-vector-data" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="loading-vector-data"><span class="header-section-number">3.2.2</span> Loading vector data</h4>
<p>Similar to loading raster data, vector data can be loaded very simply using the terra <code>vect()</code> function. The resulting vector data is a “spatVect” object. When loading shapefiles, which consist of several related files in a standard structure, only the .shp file needs to be called, however any missing files not present in the given shapefile directory may cause unexpected errors.</p>
<pre><code># Load shapefile for clipping data extent
# Example = Maricopa county, AZ
shp_path &lt;- list.files("Shapefile",  # Relative file path
                      # pattern = regex, ".shp" at end of string.
                      pattern = ".shp$",  
                      # Full file path
                      full.names = T)
shp &lt;- vect(shp_path)  # terra spatVector object
plot(shp)  # Check if data loaded correctly</code></pre>
</section>
<section id="data-projections" class="level4" data-number="3.2.3">
<h4 data-number="3.2.3" class="anchored" data-anchor-id="data-projections"><span class="header-section-number">3.2.3</span> Data projections</h4>
<p><strong>IMPORTANT!</strong> Geospatial data projections are a complex and highly technical field, related to the 2-dimensional representation of 3-dimensional objects. While I will refrain from a long diatrive on map projections, suffice to say that projections are critical for mapping as well as ensuring accurate measurements. Unlike other commercial GIS software, terra will not perform “on the fly” projections, so all geospatial data must use a similar projection to be mapped appropriately.</p>
<p>Here we will use an Arizona State Plane, Central (ft, EPSG:2223), which is appropriate for Maricopa county, AZ. For future projects discuss appropriate data projections with your PI or GIS manager. See https://epsg.io/ to query EPSG codes for your desired projection.</p>
<pre><code># Project Data
# IMPORTANT!
# Data projections control the planar (2d) shape of your map, projected from a spherical
#   (3d) space. Projections are very important for distance and area geospatial calculations
#   and will vary based on project and study area. Additionally, projections between data
#   layers must match so that they will correctly align when plotting. Terra will not 
#   perform "on the fly" projections.
# You can check your data layer projections using the terra::crs() function.
crs(rast1)  # PRISM raster(s) projection
crs(shp)  # Projection of your shapefile
# You can ignore most of the projection information here, but notice that the projections
#   do not match.
setequal(crs(rast), crs(shp))  # check if objects are equal
# For this tutorial, I will use AZ State Planes, Central projection, which is appropriate
#   for Maricopa County, AZ (EPSG:2223). Consult your PI or GIS technician for help selecting
#   the appropriate projection for your research.
projection = "EPSG:2223"
# For additional help with projections see:
#   https://epsg.io/

# Reproject data
# Reproject shp using EPSG code, overwrite shp
shp &lt;- project(shp, projection)  # AZ State Plane, Central (ft)
# Reproject raster 1 to match with shp, overwrite rast1
rast1 &lt;- project(rast1, crs(shp))
# Check projections
crs(shp)
crs(rast1)
# setequal(shp, rast1)  # I am not sure why this is returning F, but the projections are matching...</code></pre>
<p>Now that we have set our data projection we can map our data layers together on a single map. First we will map the full PRISM dataset covering the whole contiguous US, adding Maricopa county on top of our base layer.</p>
<pre><code># Check projections and layer alignment
plot(rast1, main = name)  # USA will look skewed in "Arizona Central" projection
plot(shp, add = T)  # Shape will appear small on USA map, position should be correct
# If the position of your shapefile is incorrect, or if the shapefile does not appear on
#   the map, please double check your projection and try again.
# Depending on the background color and size of your study area your shapefile may be 
#   difficult to see at this stage.</code></pre>
<p>Notice how strange the US looks in this projection? This is because our projection is optimized for central AZ. It is highly accurate in that area, and the further away from that region the data is, the more skewed the data becomes.</p>
<p>Now lets map the data using only the extent of our shapefile, but without croping the data layer. This is equivalent of setting the x and y scales in a typical scatterplot, without modifying the underlying data.</p>
<pre><code># Define study area extent, plot
e &lt;- ext(shp)  # extent of your shapefile
# Plot PRISM raster with limited extent
plot(rast1, ext = e, main = name)
plot(shp, add = T)
# Your shapefile should be visible as a thin outline.</code></pre>
</section>
<section id="crop-the-data" class="level4" data-number="3.2.4">
<h4 data-number="3.2.4" class="anchored" data-anchor-id="crop-the-data"><span class="header-section-number">3.2.4</span> Crop the data</h4>
<p>Now that we have set our projection and we have an extent that we are happy with we can crop our data layer.</p>
<pre><code># Crop raster, plot
rast_crop &lt;- crop(rast1, 1.05 * e)  # Add 5% margin to extent to avoid clipping vertices
plot(rast_crop, main = name)
plot(shp, add = T)</code></pre>
</section>
<section id="save-the-output" class="level4" data-number="3.2.5">
<h4 data-number="3.2.5" class="anchored" data-anchor-id="save-the-output"><span class="header-section-number">3.2.5</span> Save the output</h4>
<p>Create a new directory for outputs and use the <code>writeRaster()</code> function to save the output.</p>
<pre><code># Create directory for modified outputs
dir.create("Output", recursive = T)  # Ignore warning if dir already exists
writeRaster(rast_crop, filename = "Output/test_case.tif",
            overwrite = T)</code></pre>
<p>You should now see a new “Output” folder in your wd, with a new “test_case.tif” file in it. In the next section we will automate the above processing, and we will also make a few tweaks to make sure we maintain the correct PRISM file structure, as well as all the associated metadata.</p>
</section>
</section>
<section id="automated-data-processing" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="automated-data-processing"><span class="header-section-number">3.3</span> Automated data processing</h3>
<p>In this section we will use some complex logic to bulk process all of the data files in our “Data” folder. The raster files will be reprojected and cropped to our area of interest, and the outputs will be saved in the “Output” folder, while preserving all of the PRISM data file naming conventions and file tree. All metadata will be copied to the new directory, and (optionally) the raw data will be erased.</p>
<p>While it is recommended that the PRISM raw data files are deleted, the data files may not be recoverable through the recycling bin. Therefore, <strong>please be sure that you do not need to retain the raw data before proceeding, as repreated downloading of data from the PRISM api may not be possible</strong>.</p>
<pre><code># Optional vars
retainRaw &lt;- F  # Keep raw data (full USA extent)?</code></pre>
<pre><code># Crop all Datasets ----
# Now that we have explored our data and tested our algorithm we can automate the cropping
#   of all the remaining files.
# Setup
# list directories
dirs &lt;- list.dirs("Data", recursive = F)
dirs

system.time({  # Timing function
  # Load, crop, save, looping function
  # For directory in list of directories
  for(dir in dirs) {
    i &lt;- which(dirs == dir) # Get index number in dirs using pattern matching
    # Progress Statement
    print(paste("Processing file #", i, " of ", length(dirs), ".", sep = ""))
    
    # Explore dir
    files &lt;- list.files(path = dir)  # Get dir contents
    # Drop extension from file, for output dir
    name &lt;- strsplit(files[1], split = "\\.")[[1]][1]  # Split sting on literal "."
    # Create output directory
    dir.create(paste("Output/", name, sep = "/"), recursive = T)
   
    # For each file in list of files
    for (file in files) {
      # If file is .bil (raster data)
      if (grepl("bil.bil$", file) == T) {
        # Process raster data
        r &lt;- rast(paste(dir, file, sep = "/")) # load raster data
        r_proj &lt;- project(r, projection) # project
        r_crop &lt;- crop(r_proj, ext(1.05 * ext(shp))) # crop

        # Write cropped raster data
        # Preserve file structure and naming conventions from PRISM
        writeRaster(r_crop, 
          filename = paste("Output/", name, "/", name, ".bil", sep = ""), 
          filetype = "EHdr",  # Esri .bil format
          overwrite = T)  # Enable overwriteing
      } else {  # File metadata
        metadataFile = list.files(dir, pattern = file, full.names = T)
        # Copy metadata into output dir)
        file.copy(metadataFile,
          paste("Output/", name, sep = ""))
      }
    }  # end for file in files

    if (retainRaw == F) {
      # Delete raw data
      unlink(dir, recursive = T)
    }
  }  # end for dir in dirs
})  # end sys.time</code></pre>
<p>Processing for this small exampl takes approximately 13 seconds.</p>
</section>
<section id="sec-cropData" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="sec-cropData"><span class="header-section-number">3.4</span> Checking the cropped data</h3>
<p>As with the raw data above, we will check the contents of our cropped data in the “Output” directory and look at how much space we have saved by cropping our data. If you restarted your R session at the beginning of <a href="#sec-terra" class="quarto-xref">Section&nbsp;3</a> then you may need to rerun the <code>dir_size()</code> function in <a href="#sec-rawData" class="quarto-xref">Section&nbsp;2.5</a>.</p>
<pre><code># Optional: Check size of Output files
croppedDataSize &lt;- dir_size("Output")/10**6
cat(paste("Output folder size on disk =", croppedDataSize , "MB", sep = " "))
storageSaved &lt;- rawDataSize - croppedDataSize
cat(paste("Space saved on disk by cropping files =", storageSaved, "MB", sep = " "))</code></pre>
<p>In this example we reduced our file storage from over 100 MB to less than 21 MB, a savings of nearly 80%!</p>
</section>
</section>
<section id="conclusions" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">4</span> Conclusions</h2>
<p>In this simple example I have shown you how to efficiently download bulk climactic data from the PRISM api using the prism R package. I also briefly introduced the terra package, and showed you how to explore and map geospatial data. Then we used a simple code block to automate the bulk processing of raw data for our area of interest, saving ~80% of file storage requirements.</p>
<p>Hopefully by now you feel confident in being able to apply these methods to your own study area. By updating the shapefile, and modifying a few key variables you can efficiently acquire climactic data for a broad range of applications anywhere in the contiguous US. Using the R script accompanying this tutorial you can wow your supervisors and be the envy of your peers with how simply and efficiently you are able to access the data you need.</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" role="list">
<div id="ref-base_R" class="csl-entry" role="listitem">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">R Core Team (2024) <a href="https://www.R-project.org/">R: A language and environment for statistical computing</a>. R Foundation for Statistical Computing, Vienna, Austria</div>
</div>
<div id="ref-prism" class="csl-entry" role="listitem">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Hart EM, Bell K (2015) <a href="https://doi.org/10.5281/zenodo.33663">Prism: Download data from the oregon prism project</a></div>
</div>
<div id="ref-lubridate" class="csl-entry" role="listitem">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Grolemund G, Wickham H (2011) <a href="https://www.jstatsoft.org/v40/i03/">Dates and times made easy with <span class="nocase">lubridate</span></a>. Journal of Statistical Software 40(3):1–25</div>
</div>
<div id="ref-terra" class="csl-entry" role="listitem">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Hijmans RJ (2024) <a href="https://CRAN.R-project.org/package=terra">Terra: Spatial data analysis</a></div>
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>